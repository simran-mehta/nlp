{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08517813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce6673a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\") #to create nlp as object that has english understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9557520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp is an object who has general english understanding, blank english language object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fd1f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Dr. strange loves pav bhaji of mumbai as it costs only 2$ per plate. Hulk loves chaat of delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "566010f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "strange\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "mumbai\n",
      "as\n",
      "it\n",
      "costs\n",
      "only\n",
      "2\n",
      "$\n",
      "per\n",
      "plate\n",
      ".\n",
      "Hulk\n",
      "loves\n",
      "chaat\n",
      "of\n",
      "delhi\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cadfe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strange\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(doc)\n",
    "print(doc[1])\n",
    "type(doc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e936184e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc058f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr. strange loves pav bhaji of mumbai as it costs only 2$ per plate. Hulk loves chaat of delhi"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f9c2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Let's go to N.Y.!\")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1868835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[1:5]\n",
    "type(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba848363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Let"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = doc[0]\n",
    "token0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad7addca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6462e055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad775da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Let's buy 500 $ ipad and 2 bananas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79adc8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2 = doc[3]\n",
    "print(token2)\n",
    "token2.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c648e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2 = doc[4]\n",
    "print(token2)\n",
    "\n",
    "token2.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8074a89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let ==> index:  0 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "'s ==> index:  1 is_alpha: False is_punct: False like_num: False is_currency: False\n",
      "buy ==> index:  2 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "500 ==> index:  3 is_alpha: False is_punct: False like_num: True is_currency: False\n",
      "$ ==> index:  4 is_alpha: False is_punct: False like_num: False is_currency: True\n",
      "ipad ==> index:  5 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "and ==> index:  6 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "2 ==> index:  7 is_alpha: False is_punct: False like_num: True is_currency: False\n",
      "bananas ==> index:  8 is_alpha: True is_punct: False like_num: False is_currency: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"==>\", \"index: \", token.i, \"is_alpha:\", token.is_alpha, \n",
    "          \"is_punct:\", token.is_punct, \n",
    "          \"like_num:\", token.like_num,\n",
    "          \"is_currency:\", token.is_currency,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a4b0210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"students.txt\") as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89134e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n \\n \\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" \".join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2637bb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virat@kohli.com',\n",
       " 'maria@sharapova.com',\n",
       " 'serena@williams.com',\n",
       " 'joe@root.com']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "emails     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b457163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy token attributie like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d86405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9eac1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fsdfj jfsdjlf , sdklfj ', ' dkjsl   ']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"fsdfj jfsdjlf , sdklfj . dkjsl   \"\n",
    "l = s.split(\".\")\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fb5f7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb2742ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fsdfj', 'jfsdjlf', ',', 'sdklfj', '.', 'dkjsl', '', '', '']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"fsdfj jfsdjlf , sdklfj . dkjsl   \"  #string to list\n",
    "l = s.split(\" \")\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "119213f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, list)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l), type(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b96af36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fsdfj jfsdjlf , sdklfj . dkjsl   '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(l)  #list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4dd4d871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fsdfj.jfsdjlf.,.sdklfj...dkjsl...'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\".\".join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3da49edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model in Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aff5ddc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "क्ऐसए False\n",
      "हो False\n",
      "मेरे False\n",
      "5000 False\n",
      "रु False\n",
      "वपिस False\n",
      "दो False\n",
      "मुझे False\n",
      "निन्द False\n",
      "आरि False\n",
      "ह False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"hi\")\n",
    "\n",
    "doc = nlp(\"क्ऐसए हो मेरे 5000 रु वपिस दो मुझे निन्द आरि ह\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.is_currency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76fd5867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "भैया False\n",
      "जी False\n",
      "! False\n",
      "5000 False\n",
      "₹ True\n",
      "उधार False\n",
      "थे False\n",
      "वो False\n",
      "वापस False\n",
      "देदो False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"hi\")\n",
    "doc = nlp(\"भैया जी! 5000 ₹ उधार थे वो वापस देदो\")\n",
    "for token in doc:\n",
    "    print(token, token.is_currency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00d707aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b91a6700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tokenizer.add_special_case(\"gimme\", [\n",
    "    {ORTH: \"gim\"},\n",
    "    {ORTH: \"me\"},\n",
    "])\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9885b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can to give me you can't modify -> it just split the segments and can't change the text in tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "340174c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m### Sentence Tokenization or Segmentation\u001b[39;00m\n\u001b[0;32m      3\u001b[0m doc \u001b[39m=\u001b[39m nlp(\u001b[39m\"\u001b[39m\u001b[39mDr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msents:\n\u001b[0;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(sentence)\n",
      "File \u001b[1;32mc:\\nlp_codebasics\\codebasics\\nlp\\nlp_environment\\lib\\site-packages\\spacy\\tokens\\doc.pyx:892\u001b[0m, in \u001b[0;36msents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "### Sentence Tokenization or Segmentation\n",
    "\n",
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4529b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9bac210c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x15b673bcf80>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bef4b75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2cc4dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav bhaji of mumbai.\n",
      "Hulk loves chat of delhi\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5709680a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.google.com\n",
      "www.apple.com\n",
      "www.amazon.com\n",
      "www.microsoft.com\n",
      "www.facebook.com\n",
      "www.twitter.com\n",
      "www.instagram.com\n",
      "www.linkedin.com\n",
      "www.uber.com\n",
      "www.airbnb.com\n",
      "www.netflix.com\n",
      "www.ebay.com\n",
      "www.paypal.com\n",
      "www.tesla.com\n",
      "www.dropbox.com\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('''Google, Apple, Amazon, Microsoft, Facebook, Twitter, Instagram, LinkedIn, Uber, Airbnb, Netflix, eBay, PayPal, Tesla, and Dropbox are all popular companies with unique domain names for their websites. Google's domain name is www.google.com, while Apple's is www.apple.com. Amazon can be found at www.amazon.com and Microsoft's website is located at www.microsoft.com. Facebook's domain name is www.facebook.com and Twitter's is www.twitter.com, while Instagram can be accessed at www.instagram.com and LinkedIn at www.linkedin.com. Uber's website is www.uber.com, and Airbnb can be found at www.airbnb.com. Netflix's domain name is www.netflix.com, eBay's is www.ebay.com, and PayPal's is www.paypal.com. Finally, Tesla's website can be accessed at www.tesla.com and Dropbox's at www.dropbox.com.\n",
    "''')\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_url:\n",
    "        print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "484e0dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is - www.paypal.com\n",
      "at - www.dropbox.com\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('''Google, Apple, Amazon, Microsoft, Facebook, Twitter, Instagram, LinkedIn, Uber, Airbnb, Netflix, eBay, PayPal, Tesla, and Dropbox are all popular companies with unique domain names for their websites. Google's domain name is www.google.com, while Apple's is www.apple.com. Amazon can be found at www.amazon.com and Microsoft's website is located at www.microsoft.com. Facebook's domain name is www.facebook.com and Twitter's is www.twitter.com, while Instagram can be accessed at www.instagram.com and LinkedIn at www.linkedin.com. Uber's website is www.uber.com, and Airbnb can be found at www.airbnb.com. Netflix's domain name is www.netflix.com, eBay's is www.ebay.com, and PayPal's is www.paypal.com. Finally, Tesla's website can be accessed at www.tesla.com and Dropbox's at www.dropbox.com.\n",
    "''')\n",
    "\n",
    "websites = {}\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_url:\n",
    "        # Find the previous token which is the company name\n",
    "        company_name = token.nbor(-1).text\n",
    "        # Add the company name and domain name to the dictionary\n",
    "        websites[company_name] = token.text\n",
    "        \n",
    "for company, domain in websites.items():\n",
    "    print(f\"{company} - {domain}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9be6cf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain - www.netflix.com\n",
      "Apple - www.apple.com\n",
      "be - www.tesla.com\n",
      "is - www.microsoft.com\n",
      "Twitter - www.twitter.com\n",
      "and - www.linkedin.com\n",
      "'s - www.uber.com\n",
      "eBay - www.ebay.com\n",
      "PayPal - www.paypal.com\n",
      "Dropbox - www.dropbox.com\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('''Google, Apple, Amazon, Microsoft, Facebook, Twitter, Instagram, LinkedIn, Uber, Airbnb, Netflix, eBay, PayPal,\n",
    " Tesla, and Dropbox are all popular companies with unique domain names for their websites.\n",
    "   Google's domain name is www.google.com, while Apple's is www.apple.com.\n",
    "     Amazon can be found at www.amazon.com and Microsoft's website is located at www.microsoft.com.\n",
    "       Facebook's domain name is www.facebook.com and Twitter's is www.twitter.com,\n",
    "         while Instagram can be accessed at www.instagram.com and LinkedIn at www.linkedin.com.\n",
    "           Uber's website is www.uber.com, and Airbnb can be found at www.airbnb.com.\n",
    "             Netflix's domain name is www.netflix.com, eBay's is www.ebay.com, and PayPal's is www.paypal.com.\n",
    "               Finally, Tesla's website can be accessed at www.tesla.com and Dropbox's at www.dropbox.com.\n",
    "''')\n",
    "\n",
    "websites = {}\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_url:\n",
    "        # Find the previous token which is the company name\n",
    "        company_name = token.nbor(-3).text\n",
    "        # Add the company name and domain name to the dictionary\n",
    "        websites[company_name] = token.text\n",
    "        \n",
    "for company, domain in websites.items():\n",
    "    print(f\"{company} - {domain}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d2008de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = '''Google, Apple, Amazon, Microsoft, Facebook, Twitter, Instagram, LinkedIn, Uber, Airbnb, Netflix, eBay, PayPal, Tesla, and Dropbox are all popular companies with unique domain names for their websites. Google's domain name is www.google.com, while Apple's is www.apple.com. Amazon can be found at www.amazon.com and Microsoft's website is located at www.microsoft.com. Facebook's domain name is www.facebook.com and Twitter's is www.twitter.com, while Instagram can be accessed at www.instagram.com and LinkedIn at www.linkedin.com. Uber's website is www.uber.com, and Airbnb can be found at www.airbnb.com. Netflix's domain name is www.netflix.com, eBay's is www.ebay.com, and PayPal's is www.paypal.com. Finally, Tesla's website can be accessed at www.tesla.com and Dropbox's at www.dropbox.com.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49ad762c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google - www.google.com\n",
      "Apple - www.apple.com\n",
      "Amazon - www.amazon.com\n",
      "Microsoft - www.microsoft.com\n",
      "Facebook - www.facebook.com\n",
      "Twitter - www.twitter.com\n",
      "Instagram - www.instagram.com\n",
      "Linkedin - www.linkedin.com\n",
      "Uber - www.uber.com\n",
      "Airbnb - www.airbnb.com\n",
      "Netflix - www.netflix.com\n",
      "Ebay - www.ebay.com\n",
      "Paypal - www.paypal.com\n",
      "Tesla - www.tesla.com\n",
      "Dropbox - www.dropbox.com\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(Text)\n",
    "\n",
    "websites = {}\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_url:\n",
    "        # Extract the company name from the domain name\n",
    "        domain = token.text\n",
    "        company_name = domain.split(\".\")[1].capitalize()\n",
    "        # Add the company name and domain name to the dictionary\n",
    "        websites[company_name] = domain\n",
    "        \n",
    "for company, domain in websites.items():\n",
    "    print(f\"{company} - {domain}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1cec3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"Today, I exchanged my US Dollars ($1000) for some Euro (€850), Japanese Yen (¥108800), British Pounds (£750), and Swiss Francs (CHF1100). Later, I used some Canadian Dollars (C$800) to buy some Australian Dollars ($600) and Mexican Pesos ($2000). Finally, I converted some Indian Rupees (₹5000) into Chinese Yuan (¥500). It's fascinating to see the variety of currencies with their unique symbols and values in the world!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa9e4ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\n",
      "1000\n",
      "€\n",
      "850\n",
      "¥\n",
      "108800\n",
      "£\n",
      "750\n",
      "800\n",
      "$\n",
      "600\n",
      "$\n",
      "2000\n",
      "₹\n",
      "5000\n",
      "¥\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.like_num or token.is_currency:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "568fd3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US Dollar=$1000\n",
      "Euro=€850\n",
      "Japanese Yen=¥108800\n",
      "British Pound=£750\n",
      "US Dollar=$600\n",
      "US Dollar=$2000\n",
      "Indian Rupee=₹5000\n",
      "Japanese Yen=¥500\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(Text)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "currency_dict = {\n",
    "    \"$\": \"US Dollar\",\n",
    "    \"€\": \"Euro\",\n",
    "    \"¥\": \"Japanese Yen\",\n",
    "    \"£\": \"British Pound\",\n",
    "    \"CHF\": \"Swiss Franc\",\n",
    "    \"C$\": \"Canadian Dollar\",\n",
    "    \"AUD\": \"Australian Dollar\",\n",
    "    \"MXN\": \"Mexican Peso\",\n",
    "    \"₹\": \"Indian Rupee\",\n",
    "    \"CNY\": \"Chinese Yuan\"\n",
    "}\n",
    "\n",
    "for token in doc:\n",
    "    if token.is_currency:\n",
    "        print(currency_dict[str(token)] + \"=\" + str(token) + token.nbor(1).text)\n",
    "\n",
    "        # next_token = token.nbor(1).text\n",
    "        # print(next_token)\n",
    "        # Extract the company name f-rom the domain name\n",
    "#         domain = token.text\n",
    "#         company_name = domain.split(\".\")[1].capitalize()\n",
    "#         # Add the company name and domain name to the dictionary\n",
    "#         websites[company_name] = domain\n",
    "        \n",
    "# for company, domain in websites.items():\n",
    "#     print(f\"{company} - {domain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c5c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90226e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04421d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "3179a61356900d39da0ad5de14f553a170908013975f461caaaf2834e5c69eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
